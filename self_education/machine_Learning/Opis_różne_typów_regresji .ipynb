{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087587c8",
   "metadata": {},
   "source": [
    "Regresja to technika statystyczna uÅ¼ywana do modelowania i analizowania zaleÅ¼noÅ›ci miÄ™dzy zmiennymi wejÅ›ciowymi (niezaleÅ¼nymi) a zmiennymi wyjÅ›ciowymi (zaleÅ¼nymi). Celem regresji jest przewidywanie wartoÅ›ci zmiennej zaleÅ¼nej na podstawie cech wejÅ›ciowych.\n",
    "\n",
    "Modele regresji to zaleÅ¼noÅ›ci funkcjonalne miÄ™dzy cechami wejÅ›ciowymi a wartoÅ›ciami wyjÅ›ciowymi, ktÃ³re moÅ¼na przedstawiÄ‡ w postaci rÃ³wnaÅ„ matematycznych. RÃ³wnania te mogÄ… byÄ‡ liniowe lub nieliniowe, w zaleÅ¼noÅ›ci od charakteru zaleÅ¼noÅ›ci miÄ™dzy zmiennymi.\n",
    "\n",
    "IstniejÄ… rÃ³wnieÅ¼ rÃ³Å¼ne typy regresji, ktÃ³rych moÅ¼na uÅ¼yÄ‡ w zaleÅ¼noÅ›ci od charakterystyki danych i celÃ³w modelowania. NiektÃ³re z nich obejmujÄ…:\n",
    "\n",
    "1. Wielokrotna regresja liniowa: model uwzglÄ™dniajÄ…cy wiele cech wejÅ›ciowych i budujÄ…cy liniowÄ… zaleÅ¼noÅ›Ä‡ miÄ™dzy nimi a zmiennÄ… wyjÅ›ciowÄ….\n",
    "\n",
    "W wielokrotnej regresji liniowej model jest reprezentowany przez rÃ³wnanie:\n",
    "\n",
    "$ y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š,$\n",
    "\n",
    "gdzie y - zmienna zaleÅ¼na (wartoÅ›Ä‡ wyjÅ›ciowa), Î¸â‚€ - wyraz wolny (punkt wolny), Î¸â‚, Î¸â‚‚, ..., Î¸â‚š - wspÃ³Å‚czynniki regresji, xâ‚, xâ‚‚, ..., xâ‚š - zmienne niezaleÅ¼ne (cechy wejÅ›ciowe).\n",
    "\n",
    "Wielokrotna regresja liniowa uwzglÄ™dnia interakcje miÄ™dzy rÃ³Å¼nymi zmiennymi niezaleÅ¼nymi i ich wpÅ‚yw na zmiennÄ… zaleÅ¼nÄ…. WspÃ³Å‚czynniki regresji (Î¸â‚, Î¸â‚‚, ..., Î¸â‚š) okreÅ›lajÄ… udziaÅ‚ kaÅ¼dej zmiennej niezaleÅ¼nej w zmiennej zaleÅ¼nej, gdy wszystkie inne zmienne sÄ… brane pod uwagÄ™.\n",
    "\n",
    "Podczas korzystania z wielokrotnej regresji liniowej naleÅ¼y wziÄ…Ä‡ pod uwagÄ™ zaÅ‚oÅ¼enia modelu, takie jak liniowoÅ›Ä‡, niezaleÅ¼noÅ›Ä‡ od bÅ‚Ä™dÃ³w, staÅ‚a wariancja bÅ‚Ä™dÃ³w i rozkÅ‚ad normalny bÅ‚Ä™dÃ³w. Przydatne moÅ¼e byÄ‡ rÃ³wnieÅ¼ zastosowanie metod oceny istotnoÅ›ci wspÃ³Å‚czynnikÃ³w regresji, takich jak testy t i analiza wariancji (ANOVA).\n",
    "\n",
    "Aby oszacowaÄ‡ parametry w wielokrotnej regresji liniowej, moÅ¼na uÅ¼yÄ‡ metody najmniejszych kwadratÃ³w (OLS) lub metody spadku gradientu, podobnie jak w przypadku prostej regresji liniowej.\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. WszechstronnoÅ›Ä‡\n",
    "2. InterpretowalnoÅ›Ä‡\n",
    "3. ZrÃ³wnowaÅ¼ony rozwÃ³j\n",
    "\n",
    "Wady \n",
    "\n",
    "1. LiniowoÅ›Ä‡\n",
    "2. WielowspÃ³Å‚liniowoÅ›Ä‡\n",
    "3. ZaÅ‚oÅ¼enia: Wielokrotna regresja liniowa wymaga przyjÄ™cia pewnych zaÅ‚oÅ¼eÅ„, takich jak rozkÅ‚ad normalny reszt, homoskedastycznoÅ›Ä‡ i liniowoÅ›Ä‡ zaleÅ¼noÅ›ci. JeÅ›li te zaÅ‚oÅ¼enia nie sÄ… speÅ‚nione, wyniki modelu mogÄ… byÄ‡ niewiarygodne.\n",
    "\n",
    "\n",
    "2. Regresja wielomianowa: model, ktÃ³ry modeluje zwiÄ…zek miÄ™dzy cechami wejÅ›ciowymi a zmiennÄ… wyjÅ›ciowÄ… przy uÅ¼yciu funkcji wielomianu wyÅ¼szego stopnia.\n",
    "\n",
    "W regresji liniowej model jest reprezentowany przez rÃ³wnanie:\n",
    "\n",
    "y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š,\n",
    "\n",
    "gdzie y - zmienna zaleÅ¼na, Î¸â‚€, Î¸â‚, Î¸â‚‚, ..., Î¸â‚š - wspÃ³Å‚czynniki regresji, xâ‚, xâ‚‚, ..., xâ‚š - zmienne niezaleÅ¼ne.\n",
    "\n",
    "W regresji cech wielomianowych dodajemy do modelu wykÅ‚adniki oryginalnych zmiennych objaÅ›niajÄ…cych, aby uwzglÄ™dniÄ‡ zaleÅ¼noÅ›ci nieliniowe. Na przykÅ‚ad dla regresji drugiego rzÄ™du model wyglÄ…da nastÄ™pujÄ…co:\n",
    "\n",
    "y = Î¸â‚€ + Î¸â‚x + Î¸â‚‚xÂ²,\n",
    "\n",
    "gdzie x to pierwotna zmienna niezaleÅ¼na, xÂ² to jej kwadrat, Î¸â‚€, Î¸â‚, Î¸â‚‚ to wspÃ³Å‚czynniki regresji.\n",
    "\n",
    "JeÅ¼eli model regresji zawiera stopnie zmiennych wyÅ¼sze od pierwszego, to taki model nazywamy regresjÄ… wielomianowÄ….\n",
    "\n",
    "Podczas korzystania z regresji wielomianowej waÅ¼ne jest monitorowanie ponownego uczenia modelu. Wysoki stopieÅ„ wielomianu moÅ¼e prowadziÄ‡ do modelu, ktÃ³ry jest zbyt elastyczny, dobrze dostosowujÄ…c siÄ™ do danych uczÄ…cych, ale sÅ‚abo uogÃ³lniajÄ…c nowe dane.\n",
    "\n",
    "Wyboru optymalnego stopnia wielomianu i kontroli przeuczenia moÅ¼na dokonaÄ‡ za pomocÄ… metod regularyzacji (np. regularyzacji L1 i L2), walidacji krzyÅ¼owej i analizy resztkowej.\n",
    "\n",
    "WzÃ³r na wielokrotnÄ… regresjÄ™ liniowÄ… z wykorzystaniem funkcji wielomianowych jest nastÄ™pujÄ…cy:\n",
    "\n",
    "y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š + Î¸â‚šâ‚Šâ‚xâ‚Â² + Î¸â‚šâ‚Šâ‚‚xâ‚xâ‚‚ + ... + Î¸â‚‚â‚šxâ‚šÂ² + ...,\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "y - zmienna zaleÅ¼na (wartoÅ›Ä‡ wyjÅ›ciowa),\n",
    "Î¸â‚€ - przeciÄ™cie (przeciÄ™cie),\n",
    "Î¸â‚, Î¸â‚‚, ..., Î¸â‚š - wspÃ³Å‚czynniki regresji dla cech liniowych xâ‚, xâ‚‚, ..., xâ‚š,\n",
    "Î¸â‚šâ‚Šâ‚, Î¸â‚šâ‚Šâ‚‚, ..., Î¸â‚‚â‚š - wspÃ³Å‚czynniki regresji dla cech kwadratowych xâ‚Â², xâ‚xâ‚‚, ..., xâ‚šÂ²,\n",
    "i tak dalej, w tym cechy wyÅ¼szych rzÄ™dÃ³w.\n",
    "\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. ElastycznoÅ›Ä‡ modelu:\n",
    "1. Najlepsze dopasowanie danych\n",
    "1. MoÅ¼liwoÅ›Ä‡ przechwytywania interakcji\n",
    "\n",
    "Wady \n",
    "\n",
    "1. RosnÄ…ca zÅ‚oÅ¼onoÅ›Ä‡ modelu\n",
    "1. Ryzyko wspÃ³Å‚liniowoÅ›ci\n",
    "1. RosnÄ…ca zÅ‚oÅ¼onoÅ›Ä‡ obliczeniowa\n",
    "\n",
    "\n",
    "3. Regresja logistyczna: model uÅ¼ywany do klasyfikacji binarnej, ktÃ³ry przewiduje prawdopodobieÅ„stwo przynaleÅ¼noÅ›ci do jednej z dwÃ³ch klas.\n",
    "\n",
    "WzÃ³r na regresjÄ™ logistycznÄ… jest nastÄ™pujÄ…cy:\n",
    "\n",
    "p(y=1 | x; Î¸) = sigmoid(Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "p(y=1 | x; Î¸) - prawdopodobieÅ„stwo, Å¼e zmienna zaleÅ¼na y jest rÃ³wna 1 dla danych wartoÅ›ci zmiennych niezaleÅ¼nych x i parametrÃ³w modelu Î¸,\n",
    "\n",
    "sigmoid(z) - funkcja sigmoidalna (funkcja logistyczna)\n",
    "\n",
    "zdefiniowana jako sigmoid(z) = 1 / (1 + exp(-z)),\n",
    "\n",
    "Î¸â‚€ - przeciÄ™cie ,\n",
    "\n",
    "Î¸â‚, Î¸â‚‚, ..., Î¸â‚š - wspÃ³Å‚czynniki regresji dla zmiennych niezaleÅ¼nych xâ‚, xâ‚‚, ..., xâ‚š.\n",
    "\n",
    "Regresja logistyczna wykorzystuje funkcjÄ™ sigmoidalnÄ… do konwersji liniowej kombinacji parametrÃ³w i zmiennych objaÅ›niajÄ…cych na prawdopodobieÅ„stwo przynaleÅ¼noÅ›ci do klasy 1. Funkcja sigmoidalna ogranicza prawdopodobieÅ„stwa miÄ™dzy 0 a 1.\n",
    "\n",
    "Do klasyfikacji zwykle stosuje siÄ™ wartoÅ›Ä‡ progowÄ… (prÃ³g), na przykÅ‚ad 0,5. JeÅ›li prawdopodobieÅ„stwo p(y=1 | x; Î¸) jest wiÄ™ksze lub rÃ³wne wartoÅ›ci progowej, wÃ³wczas przewidywana jest klasa 1, w przeciwnym razie przewidywana jest klasa 0.\n",
    "\n",
    "Szacowanie parametrÃ³w modelu (Î¸) w regresji logistycznej moÅ¼na przeprowadziÄ‡ przy uÅ¼yciu metody najwiÄ™kszej wiarygodnoÅ›ci lub innych metod optymalizacyjnych, takich jak zejÅ›cie gradientu.\n",
    "\n",
    "NaleÅ¼y zauwaÅ¼yÄ‡, Å¼e regresja logistyczna jest algorytmem klasyfikacji binarnej, to znaczy jest przeznaczona do rozwiÄ…zywania problemÃ³w, w ktÃ³rych zmienna zaleÅ¼na przyjmuje dwie klasy (zwykle 0 i 1). W przypadku klasyfikacji wieloklasowej regresjÄ™ logistycznÄ… moÅ¼na rozszerzyÄ‡, na przykÅ‚ad stosujÄ…c metodÄ™ â€jeden na wszystkichâ€ (one-vs-all) lub metodÄ™ â€jeden na jednegoâ€ (one-vs-one).\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. InterpretowalnoÅ›Ä‡\n",
    "1. Interpretacja probabilistyczna\n",
    "1. Radzenie sobie z niezrÃ³wnowaÅ¼onymi danymi\n",
    "\n",
    "Wady \n",
    "\n",
    "1. ZaÅ‚oÅ¼enia liniowe\n",
    "1. WraÅ¼liwoÅ›Ä‡ na wartoÅ›ci odstajÄ…ce\n",
    "1. ZaleÅ¼noÅ›Ä‡ od zaÅ‚oÅ¼eÅ„: Regresja logistyczna zakÅ‚ada, Å¼e dane sÄ… niezaleÅ¼ne i rÃ³wno rozÅ‚oÅ¼one\n",
    "1. Ryzyko przetrenowania\n",
    "\n",
    "\n",
    "4. Uregularyzowana regresja: Modele, ktÃ³re zawierajÄ… kary w funkcji straty, aby zapobiec nadmiernemu dopasowaniu i poprawiÄ‡ zdolnoÅ›Ä‡ uogÃ³lniania modelu. PrzykÅ‚ady obejmujÄ… regresjÄ™ grzbietowÄ… i regresjÄ™ lassowÄ….\n",
    "\n",
    "WzÃ³r na regularyzowanÄ… regresjÄ™, takÄ… jak regresja Ridge (L2; grzbietu) lub regresja lasso(l1), jest nastÄ™pujÄ…cy:\n",
    "\n",
    "Ridge(l2) Regresja:\n",
    "y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š + Î± * (Î¸â‚Â² + Î¸â‚‚Â² + ... + Î¸â‚šÂ²),\n",
    "\n",
    "Lasso(l1) Regresja :\n",
    "y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š + Î± * (|Î¸â‚| + |Î¸â‚‚| + ... + |Î¸â‚š|),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "y - zmienna zaleÅ¼na (wartoÅ›Ä‡ wyjÅ›ciowa),\n",
    "Î¸â‚€ - przeciÄ™cie (przeciÄ™cie),\n",
    "Î¸â‚, Î¸â‚‚, ..., Î¸â‚š - wspÃ³Å‚czynniki regresji dla zmiennych niezaleÅ¼nych xâ‚, xâ‚‚, ..., xâ‚š,\n",
    "Î± to parametr regularyzacji, ktÃ³ry kontroluje siÅ‚Ä™ regularyzacji i jest dostrojony w celu uzyskania optymalnego modelu,\n",
    "Î¸â‚Â², Î¸â‚‚Â², ..., Î¸â‚šÂ² - kwadratowe wspÃ³Å‚czynniki regresji dla regularyzacji w regresji Ridge,\n",
    "|Î¸â‚|, |Î¸â‚‚|, ..., |Î¸â‚š| - bezwzglÄ™dne wartoÅ›ci wspÃ³Å‚czynnikÃ³w regresji dla regularyzacji w regresji Lasso.\n",
    "\n",
    "Uregularyzowana regresja sÅ‚uÅ¼y do kontrolowania nadmiernego dopasowania modelu i zmniejszenia wpÅ‚ywu wspÃ³Å‚liniowoÅ›ci (zwiÄ…zkÃ³w miÄ™dzy zmiennymi niezaleÅ¼nymi) poprzez dodanie kary do funkcji straty. Parametr regularyzacji Î± okreÅ›la wysokoÅ›Ä‡ kary: wyÅ¼sza wartoÅ›Ä‡ Î± prowadzi do silniejszej regularyzacji, natomiast mniejsza wartoÅ›Ä‡ Î± zmniejsza efekt regularyzacji.\n",
    "\n",
    "Regresja Ridge dodaje czÅ‚on karny oparty na kwadratach wspÃ³Å‚czynnikÃ³w, co pomaga zmniejszyÄ‡ wartoÅ›ci wspÃ³Å‚czynnikÃ³w, ale nie eliminuje ich caÅ‚kowicie.\n",
    "\n",
    "Regresja Lasso dodaje termin kary na podstawie wartoÅ›ci bezwzglÄ™dnych wspÃ³Å‚czynnikÃ³w, co moÅ¼e skutkowaÄ‡ wartoÅ›ciami zerowymi dla niektÃ³rych wspÃ³Å‚czynnikÃ³w, co pozwala na selekcjÄ™ cech.\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. Zmniejszone nadmierne dopasowanie:\n",
    "1. UogÃ³lnienie poprawy umiejÄ™tnoÅ›ci\n",
    "1. WybÃ³r najwaÅ¼niejszych predyktorÃ³w\n",
    "\n",
    "Wady \n",
    "\n",
    "1. Utrata moÅ¼liwoÅ›ci interpretacji.\n",
    "1. ZaleÅ¼noÅ›Ä‡ dostrajania hiperparametrÃ³w\n",
    "1. Utrata precyzji\n",
    "\n",
    "\n",
    "5. Regresja nieliniowa: modele modelujÄ…ce nieliniowÄ… zaleÅ¼noÅ›Ä‡ miÄ™dzy cechami wejÅ›ciowymi a zmiennÄ… wyjÅ›ciowÄ… przy uÅ¼yciu nieliniowych funkcji lub algorytmÃ³w, takich jak neuron\n",
    "\n",
    "WzÃ³r na regresjÄ™ nieliniowÄ… moÅ¼e zaleÅ¼eÄ‡ od konkretnego wybranego zaÅ‚oÅ¼enia modelu nieliniowego. Jednak ogÃ³lnie wzÃ³r na regresjÄ™ nieliniowÄ… jest nastÄ™pujÄ…cy:\n",
    "\n",
    "y = f(Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "y - zmienna zaleÅ¼na (wartoÅ›Ä‡ wyjÅ›ciowa),\n",
    "\n",
    "xâ‚, xâ‚‚, ..., xâ‚š sÄ… zmiennymi niezaleÅ¼nymi,\n",
    "\n",
    "Î¸â‚, Î¸â‚‚, ..., Î¸â‚š - parametry modelu,\n",
    "\n",
    "f jest funkcjÄ… nieliniowÄ… (np. wielomianowÄ…, wykÅ‚adniczÄ…, logarytmicznÄ… itp.).\n",
    "\n",
    "W regresji nieliniowej funkcjÄ™ f moÅ¼na wybraÄ‡ zgodnie z oczekiwanÄ… zaleÅ¼noÅ›ciÄ… miÄ™dzy zmiennymi niezaleÅ¼nymi i zaleÅ¼nymi. Na przykÅ‚ad, jeÅ›li przyjmuje siÄ™ zaleÅ¼noÅ›Ä‡ wielomianowÄ…, funkcja f moÅ¼e byÄ‡ wielomianem stopnia wiÄ™kszego niÅ¼ 1. JeÅ›li przyjmuje siÄ™ zaleÅ¼noÅ›Ä‡ wykÅ‚adniczÄ…, funkcja f moÅ¼e byÄ‡ wykÅ‚adnicza. I tak dalej.\n",
    "\n",
    "NaleÅ¼y zauwaÅ¼yÄ‡, Å¼e wybÃ³r odpowiedniej funkcji nieliniowej f wymaga wstÄ™pnej analizy danych i zrozumienia natury zaleÅ¼noÅ›ci miÄ™dzy zmiennymi. Wyznaczenie odpowiedniej funkcji nieliniowej moÅ¼na przeprowadziÄ‡ empirycznie, na podstawie eksperymentÃ³w lub z wykorzystaniem metod statystycznych i uczenia maszynowego.\n",
    "\n",
    "Estymacja parametrÃ³w modelu (Î¸) w regresji nieliniowej moÅ¼e odbywaÄ‡ siÄ™ metodÄ… najmniejszych kwadratÃ³w, metodÄ… najwiÄ™kszej wiarygodnoÅ›ci lub innymi metodami optymalizacyjnymi, w zaleÅ¼noÅ›ci od wybranego modelu i preferencji badacza.\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. ElastycznoÅ›Ä‡ modelu: Regresja nieregularna umoÅ¼liwia modelowanie zÅ‚oÅ¼onych nieliniowych relacji miÄ™dzy predyktorami a zmiennÄ… zaleÅ¼nÄ….\n",
    "\n",
    "1. Lepsze dopasowanie danych: uwzglÄ™dnienie funkcji i przeksztaÅ‚ceÅ„ nieliniowych umoÅ¼liwia dokÅ‚adniejsze dopasowanie modelu do danych, zwÅ‚aszcza w przypadkach, gdy zwiÄ…zek miÄ™dzy zmiennymi jest nieliniowy.\n",
    "\n",
    "1. MoÅ¼liwoÅ›Ä‡ modelowania zÅ‚oÅ¼onych interakcji: Regresja nieregularna moÅ¼e uchwyciÄ‡ zÅ‚oÅ¼one interakcje miÄ™dzy predyktorami, co pozwala na dodatkowÄ… zÅ‚oÅ¼onoÅ›Ä‡ danych i poprawia moc predykcyjnÄ… modelu.\n",
    "\n",
    "Wady \n",
    "\n",
    "1. TrudnoÅ›Ä‡ w interpretacji: WÅ‚Ä…czenie nieliniowych cech i przeksztaÅ‚ceÅ„ komplikuje interpretacjÄ™ modelu. WspÃ³Å‚czynniki nie majÄ… juÅ¼ bezpoÅ›redniego znaczenia, jak w regresji liniowej, a ich interpretacja moÅ¼e byÄ‡ trudna.\n",
    "\n",
    "1. Nadmierne dopasowanie: w przypadku zbyt duÅ¼ej elastycznoÅ›ci modelu lub niewystarczajÄ…cych danych nieregularna regresja moÅ¼e cierpieÄ‡ z powodu nadmiernego dopasowania.\n",
    "\n",
    "1. ZÅ‚oÅ¼onoÅ›Ä‡ obliczeniowa: uwzglÄ™dnienie funkcji nieliniowych moÅ¼e zwiÄ™kszyÄ‡ zÅ‚oÅ¼onoÅ›Ä‡ obliczeniowÄ… uczenia modelu. SzczegÃ³lnie w przypadku korzystania ze zÅ‚oÅ¼onych funkcji nieliniowych lub duÅ¼ej liczby predyktorÃ³w uczenie moÅ¼e wymagaÄ‡ duÅ¼ej mocy obliczeniowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7402b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d674a4",
   "metadata": {},
   "source": [
    "Metoda najmniejszych kwadratÃ³w (OLS) to metoda statystyczna, ktÃ³ra sÅ‚uÅ¼y do oszacowania optymalnych wartoÅ›ci wspÃ³Å‚czynnikÃ³w (Î¸) w regresji liniowej. Celem OLS jest zminimalizowanie sumy kwadratÃ³w odchyleÅ„ (bÅ‚Ä™dÃ³w) miÄ™dzy wartoÅ›ciami rzeczywistymi a wartoÅ›ciami przewidywanymi modelu.\n",
    "\n",
    "Chcemy znaleÅºÄ‡ optymalne wartoÅ›ci wspÃ³Å‚czynnikÃ³w Î¸, aby zminimalizowaÄ‡ sumÄ™ bÅ‚Ä™dÃ³w kwadratowych (SSE), ktÃ³ra wyraÅ¼a siÄ™ nastÄ™pujÄ…co:\n",
    "\n",
    "$ SSE = Î£(yi - h(xi))^2 $\n",
    "\n",
    "gdzie h(xi) jest przewidywanÄ… wartoÅ›ciÄ… zaleÅ¼nÄ… od cech wejÅ›ciowych xi i wag Î¸:\n",
    "\n",
    "Gdzie $ h(xi) = Î¸0 + Î¸1x1 + Î¸2x2 + ... + Î¸n*xn $ model regresji liniowej, ktÃ³ry faktycznie przewiduje xi h(xi) to y_pred\n",
    "\n",
    "LSM pozwala uzyskaÄ‡ optymalne wartoÅ›ci wspÃ³Å‚czynnikÃ³w Î¸, ktÃ³re zapewniajÄ… najlepsze dopasowanie modelu do danych zbioru uczÄ…cego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130e14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992f3fd0",
   "metadata": {},
   "source": [
    "Gradient Descent to algorytm optymalizacji uÅ¼ywany do znalezienia minimum (lub maksimum) funkcji. Jest szeroko stosowany w uczeniu maszynowym do uczenia modeli i dostrajania ich parametrÃ³w.\n",
    "\n",
    "IdeÄ… metody zejÅ›cie gradientu jest sekwencyjne aktualizowanie wartoÅ›ci parametrÃ³w modelu w kierunku przeciwnym do gradientu funkcji strat. \n",
    "\n",
    "Gradient funkcji wskazuje kierunek najostrzejszego wzrostu funkcji, a gradient przeciwny wskazuje kierunek najszybszego spadku funkcji. Dlatego poruszanie siÄ™ w kierunku przeciwnym do gradientu pozwala zbliÅ¼yÄ‡ siÄ™ do minimum funkcji.\n",
    "\n",
    "W kontekÅ›cie regresji liniowej zejÅ›cie gradientu moÅ¼e byÄ‡ wykorzystane do dostrojenia optymalnych wartoÅ›ci parametrÃ³w (Î¸) modelu poprzez minimalizacjÄ™ sumy bÅ‚Ä™dÃ³w kwadratowych (SSE) miÄ™dzy wartoÅ›ciami rzeczywistymi i przewidywanymi. Przy kaÅ¼dej iteracji zejÅ›cie gradientu wartoÅ›ci parametrÃ³w sÄ… aktualizowane zgodnie z gradientem funkcji strat i zadanÄ… szybkoÅ›ciÄ… uczenia.\n",
    "\n",
    "Podstawowe etapy metody zejÅ›cia gradientowego to:\n",
    "1. Inicjalizacja wartoÅ›ci poczÄ…tkowych parametrÃ³w modelu.\n",
    "2. Obliczenie przewidywanych wartoÅ›ci modelu na bieÅ¼Ä…cych parametrach.\n",
    "3. Obliczanie bÅ‚Ä™dÃ³w miÄ™dzy wartoÅ›ciami przewidywanymi a rzeczywistymi.\n",
    "4. Obliczanie gradientÃ³w funkcji strat dla kaÅ¼dego parametru.\n",
    "5. Zaktualizuj wartoÅ›ci parametrÃ³w w kierunku przeciwnym do gradientu, uwzglÄ™dniajÄ…c tempo uczenia siÄ™.\n",
    "6. Powtarzaj kroki 20-50 aÅ¼ do osiÄ…gniÄ™cia warunku zatrzymania (np. osiÄ…gniÄ™cia okreÅ›lonej liczby iteracji lub wystarczajÄ…co maÅ‚ej zmiany funkcji straty).\n",
    "\n",
    "formuÅ‚a gradientu zejÅ›cie:\n",
    "\n",
    "Î¸â‚™â‚‘ğ‘¤ = Î¸â‚™â‚‘ğ‘¤ - Î± * âˆ‡J(Î¸â‚™â‚‘ğ‘¤),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "Î¸â‚™â‚‘ğ‘¤ - aktualna wartoÅ›Ä‡ parametru,\n",
    "Î± - szybkoÅ›Ä‡ uczenia siÄ™,\n",
    "âˆ‡J(Î¸â‚™â‚‘ğ‘¤) jest gradientem funkcji straty J wzglÄ™dem parametru Î¸â‚™â‚‘ğ‘¤.\n",
    "\n",
    "Gradient (âˆ‡J(Î¸â‚™â‚‘ğ‘¤)) jest wektorem zawierajÄ…cym pochodne czÄ…stkowe funkcji straty wzglÄ™dem kaÅ¼dego parametru modelu. KaÅ¼da skÅ‚adowa gradientu wskazuje kierunek najszybszego wzrostu funkcji straty w punkcie Î¸â‚™â‚‘ğ‘¤.\n",
    "\n",
    "Proces zejÅ›cie gradientu polega na iteracyjnym aktualizowaniu parametrÃ³w modelu przy uÅ¼yciu wzoru na zejÅ›cie gradientu. Na kaÅ¼dym kroku obliczamy gradient funkcji straty w stosunku do bieÅ¼Ä…cych wartoÅ›ci parametrÃ³w, nastÄ™pnie mnoÅ¼ymy go przez szybkoÅ›Ä‡ uczenia i odejmujemy wynik od bieÅ¼Ä…cych wartoÅ›ci parametrÃ³w. Idziemy wiÄ™c w kierunku najszybszego spadku funkcji straty.\n",
    "\n",
    "WybÃ³r odpowiedniego wspÃ³Å‚czynnika uczenia siÄ™ (Î±) jest waÅ¼nym aspektem zejÅ›cie gradientu. JeÅ›li Î± jest zbyt maÅ‚e, konwergencja moÅ¼e byÄ‡ powolna. JeÅ›li Î± jest zbyt duÅ¼e, zejÅ›cie moÅ¼e siÄ™ nie zbiegaÄ‡ i przekroczyÄ‡ minimum funkcji straty. WybÃ³r optymalnej wartoÅ›ci wspÃ³Å‚czynnika uczenia jest problemem optymalizacyjnym.\n",
    "\n",
    "Metoda zejÅ›cie gradientu moÅ¼e mieÄ‡ rÃ³Å¼ne odmiany, takie jak zejÅ›cie gradientu stochastycznego i zejÅ›cie gradientu mini-batch, ktÃ³re wykorzystujÄ… losowe podzbiory danych do aktualizacji parametrÃ³w w kaÅ¼dej iteracji. Te odmiany poprawiajÄ… szybkoÅ›Ä‡ konwergencji i wydajnoÅ›Ä‡ metody na duÅ¼ych iloÅ›ciach danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4e202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47f92881",
   "metadata": {},
   "source": [
    "Intercept (punkt przeciÄ™cia) w kontekÅ›cie regresji liniowej jest wolnym elementem modelu, reprezentujÄ…cym wartoÅ›Ä‡ zmiennej zaleÅ¼nej (wartoÅ›ci wyjÅ›ciowej) przy zerowych wartoÅ›ciach wszystkich cech wejÅ›ciowych.\n",
    "\n",
    "W regresji liniowej model jest reprezentowany przez rÃ³wnanie:\n",
    "\n",
    "$ y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚šxâ‚š $\n",
    "\n",
    "gdzie y - zmienna zaleÅ¼na, Î¸â‚€ - wyraz wolny, Î¸â‚, Î¸â‚‚, ..., Î¸â‚š - wspÃ³Å‚czynniki regresji, xâ‚, xâ‚‚, ..., xâ‚š - cechy wejÅ›ciowe.\n",
    "\n",
    "Punkt przeciÄ™cia (punkt przeciÄ™cia) to punkt przeciÄ™cia linii regresji z osiÄ… y przy zerowych wartoÅ›ciach wszystkich cech wejÅ›ciowych. Odzwierciedla wartoÅ›Ä‡ bazowÄ… zmiennej zaleÅ¼nej, gdy wszystkie cechy wejÅ›ciowe sÄ… zerowe.\n",
    "\n",
    "Punkt przeciÄ™cia moÅ¼e byÄ‡ dodatni lub ujemny i reprezentuje przesuniÄ™cie linii regresji w gÃ³rÄ™ lub w dÃ³Å‚ osi y.\n",
    "\n",
    "Punkt wolny odgrywa waÅ¼nÄ… rolÄ™ w modelu regresji liniowej, poniewaÅ¼ pozwala uwzglÄ™dniÄ‡ wartoÅ›Ä‡ bazowÄ… zmiennej zaleÅ¼nej, ktÃ³ra nie zaleÅ¼y od cech wejÅ›ciowych. WÅ‚Ä…czenie punktu przeciÄ™cia umoÅ¼liwia modelowanie linii regresji, ktÃ³ra niekoniecznie przechodzi przez poczÄ…tek ukÅ‚adu wspÃ³Å‚rzÄ™dnych.\n",
    "\n",
    "Punkt przeciÄ™cia (Î¸â‚€) jest jednym z parametrÃ³w estymowanych w procesie uczenia regresji liniowej i wpÅ‚ywa na ksztaÅ‚t i poÅ‚oÅ¼enie linii regresji w przestrzeni cech."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
