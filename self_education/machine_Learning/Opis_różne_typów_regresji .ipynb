{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f187ec",
   "metadata": {},
   "source": [
    "Regresja to technika statystyczna używana do modelowania i analizowania zależności między zmiennymi wejściowymi (niezależnymi) a zmiennymi wyjściowymi (zależnymi). Celem regresji jest przewidywanie wartości zmiennej zależnej na podstawie cech wejściowych.\n",
    "\n",
    "Modele regresji to zależności funkcjonalne między cechami wejściowymi a wartościami wyjściowymi, które można przedstawić w postaci równań matematycznych. Równania te mogą być liniowe lub nieliniowe, w zależności od charakteru zależności między zmiennymi.\n",
    "\n",
    "Istnieją również różne typy regresji, których można użyć w zależności od charakterystyki danych i celów modelowania. Niektóre z nich obejmują:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be019158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c497bf7",
   "metadata": {},
   "source": [
    "1. Wielokrotna regresja liniowa: model uwzględniający wiele cech wejściowych i budujący liniową zależność między nimi a zmienną wyjściową.\n",
    "\n",
    "W wielokrotnej regresji liniowej model jest reprezentowany przez równanie:\n",
    "\n",
    "$ y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ,$\n",
    "\n",
    "gdzie y - zmienna zależna (wartość wyjściowa), θ₀ - wyraz wolny (punkt wolny), θ₁, θ₂, ..., θₚ - współczynniki regresji, x₁, x₂, ..., xₚ - zmienne niezależne (cechy wejściowe).\n",
    "\n",
    "Wielokrotna regresja liniowa uwzględnia interakcje między różnymi zmiennymi niezależnymi i ich wpływ na zmienną zależną. Współczynniki regresji (θ₁, θ₂, ..., θₚ) określają udział każdej zmiennej niezależnej w zmiennej zależnej, gdy wszystkie inne zmienne są brane pod uwagę.\n",
    "\n",
    "Podczas korzystania z wielokrotnej regresji liniowej należy wziąć pod uwagę założenia modelu, takie jak liniowość, niezależność od błędów, stała wariancja błędów i rozkład normalny błędów. Przydatne może być również zastosowanie metod oceny istotności współczynników regresji, takich jak testy t i analiza wariancji (ANOVA).\n",
    "\n",
    "Aby oszacować parametry w wielokrotnej regresji liniowej, można użyć metody najmniejszych kwadratów (OLS) lub metody spadku gradientu, podobnie jak w przypadku prostej regresji liniowej.\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. Wszechstronność\n",
    "2. Interpretowalność\n",
    "3. Zrównoważony rozwój\n",
    "\n",
    "Wady \n",
    "\n",
    "1. Liniowość\n",
    "2. Wielowspółliniowość\n",
    "3. Założenia: Wielokrotna regresja liniowa wymaga przyjęcia pewnych założeń, takich jak rozkład normalny reszt, homoskedastyczność i liniowość zależności. Jeśli te założenia nie są spełnione, wyniki modelu mogą być niewiarygodne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e82744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "795b3ca5",
   "metadata": {},
   "source": [
    "2. Regresja wielomianowa: model, który modeluje związek między cechami wejściowymi a zmienną wyjściową przy użyciu funkcji wielomianu wyższego stopnia.\n",
    "\n",
    "W regresji liniowej model jest reprezentowany przez równanie:\n",
    "\n",
    "y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ,\n",
    "\n",
    "gdzie y - zmienna zależna, θ₀, θ₁, θ₂, ..., θₚ - współczynniki regresji, x₁, x₂, ..., xₚ - zmienne niezależne.\n",
    "\n",
    "W regresji cech wielomianowych dodajemy do modelu wykładniki oryginalnych zmiennych objaśniających, aby uwzględnić zależności nieliniowe. Na przykład dla regresji drugiego rzędu model wygląda następująco:\n",
    "\n",
    "y = θ₀ + θ₁x + θ₂x²,\n",
    "\n",
    "gdzie x to pierwotna zmienna niezależna, x² to jej kwadrat, θ₀, θ₁, θ₂ to współczynniki regresji.\n",
    "\n",
    "Jeżeli model regresji zawiera stopnie zmiennych wyższe od pierwszego, to taki model nazywamy regresją wielomianową.\n",
    "\n",
    "Podczas korzystania z regresji wielomianowej ważne jest monitorowanie ponownego uczenia modelu. Wysoki stopień wielomianu może prowadzić do modelu, który jest zbyt elastyczny, dobrze dostosowując się do danych uczących, ale słabo uogólniając nowe dane.\n",
    "\n",
    "Wyboru optymalnego stopnia wielomianu i kontroli przeuczenia można dokonać za pomocą metod regularyzacji (np. regularyzacji L1 i L2), walidacji krzyżowej i analizy resztkowej.\n",
    "\n",
    "Wzór na wielokrotną regresję liniową z wykorzystaniem funkcji wielomianowych jest następujący:\n",
    "\n",
    "y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ + θₚ₊₁x₁² + θₚ₊₂x₁x₂ + ... + θ₂ₚxₚ² + ...,\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "y - zmienna zależna (wartość wyjściowa),\n",
    "θ₀ - przecięcie (przecięcie),\n",
    "θ₁, θ₂, ..., θₚ - współczynniki regresji dla cech liniowych x₁, x₂, ..., xₚ,\n",
    "θₚ₊₁, θₚ₊₂, ..., θ₂ₚ - współczynniki regresji dla cech kwadratowych x₁², x₁x₂, ..., xₚ²,\n",
    "i tak dalej, w tym cechy wyższych rzędów.\n",
    "\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. Elastyczność modelu:\n",
    "1. Najlepsze dopasowanie danych\n",
    "1. Możliwość przechwytywania interakcji\n",
    "\n",
    "Wady \n",
    "\n",
    "1. Rosnąca złożoność modelu\n",
    "1. Ryzyko współliniowości\n",
    "1. Rosnąca złożoność obliczeniowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51d5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b641c09a",
   "metadata": {},
   "source": [
    "3. Regresja logistyczna: model używany do klasyfikacji binarnej, który przewiduje prawdopodobieństwo przynależności do jednej z dwóch klas.\n",
    "\n",
    "Wzór na regresję logistyczną jest następujący:\n",
    "\n",
    "p(y=1 | x; θ) = sigmoid(θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "p(y=1 | x; θ) - prawdopodobieństwo, że zmienna zależna y jest równa 1 dla danych wartości zmiennych niezależnych x i parametrów modelu θ,\n",
    "\n",
    "sigmoid(z) - funkcja sigmoidalna (funkcja logistyczna)\n",
    "\n",
    "zdefiniowana jako sigmoid(z) = 1 / (1 + exp(-z)),\n",
    "\n",
    "θ₀ - przecięcie ,\n",
    "\n",
    "θ₁, θ₂, ..., θₚ - współczynniki regresji dla zmiennych niezależnych x₁, x₂, ..., xₚ.\n",
    "\n",
    "Regresja logistyczna wykorzystuje funkcję sigmoidalną do konwersji liniowej kombinacji parametrów i zmiennych objaśniających na prawdopodobieństwo przynależności do klasy 1. Funkcja sigmoidalna ogranicza prawdopodobieństwa między 0 a 1.\n",
    "\n",
    "Do klasyfikacji zwykle stosuje się wartość progową (próg), na przykład 0,5. Jeśli prawdopodobieństwo p(y=1 | x; θ) jest większe lub równe wartości progowej, wówczas przewidywana jest klasa 1, w przeciwnym razie przewidywana jest klasa 0.\n",
    "\n",
    "Szacowanie parametrów modelu (θ) w regresji logistycznej można przeprowadzić przy użyciu metody największej wiarygodności lub innych metod optymalizacyjnych, takich jak zejście gradientu.\n",
    "\n",
    "Należy zauważyć, że regresja logistyczna jest algorytmem klasyfikacji binarnej, to znaczy jest przeznaczona do rozwiązywania problemów, w których zmienna zależna przyjmuje dwie klasy (zwykle 0 i 1). W przypadku klasyfikacji wieloklasowej regresję logistyczną można rozszerzyć, na przykład stosując metodę „jeden na wszystkich” (one-vs-all) lub metodę „jeden na jednego” (one-vs-one).\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. Interpretowalność\n",
    "1. Interpretacja probabilistyczna\n",
    "1. Radzenie sobie z niezrównoważonymi danymi\n",
    "\n",
    "Wady \n",
    "\n",
    "1. Założenia liniowe\n",
    "1. Wrażliwość na wartości odstające\n",
    "1. Zależność od założeń: Regresja logistyczna zakłada, że dane są niezależne i równo rozłożone\n",
    "1. Ryzyko przetrenowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a855c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74394345",
   "metadata": {},
   "source": [
    "4. Uregularyzowana regresja: Modele, które zawierają kary w funkcji straty, aby zapobiec nadmiernemu dopasowaniu i poprawić zdolność uogólniania modelu. Przykłady obejmują regresję grzbietową i regresję lassową.\n",
    "\n",
    "Wzór na regularyzowaną regresję, taką jak regresja Ridge (L2; grzbietu) lub regresja lasso(l1), jest następujący:\n",
    "\n",
    "Ridge(l2) Regresja:\n",
    "y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ + α * (θ₁² + θ₂² + ... + θₚ²),\n",
    "\n",
    "Lasso(l1) Regresja :\n",
    "y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ + α * (|θ₁| + |θ₂| + ... + |θₚ|),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "y - zmienna zależna (wartość wyjściowa),\n",
    "θ₀ - przecięcie (przecięcie),\n",
    "θ₁, θ₂, ..., θₚ - współczynniki regresji dla zmiennych niezależnych x₁, x₂, ..., xₚ,\n",
    "α to parametr regularyzacji, który kontroluje siłę regularyzacji i jest dostrojony w celu uzyskania optymalnego modelu,\n",
    "θ₁², θ₂², ..., θₚ² - kwadratowe współczynniki regresji dla regularyzacji w regresji Ridge,\n",
    "|θ₁|, |θ₂|, ..., |θₚ| - bezwzględne wartości współczynników regresji dla regularyzacji w regresji Lasso.\n",
    "\n",
    "Uregularyzowana regresja służy do kontrolowania nadmiernego dopasowania modelu i zmniejszenia wpływu współliniowości (związków między zmiennymi niezależnymi) poprzez dodanie kary do funkcji straty. Parametr regularyzacji α określa wysokość kary: wyższa wartość α prowadzi do silniejszej regularyzacji, natomiast mniejsza wartość α zmniejsza efekt regularyzacji.\n",
    "\n",
    "Regresja Ridge dodaje człon karny oparty na kwadratach współczynników, co pomaga zmniejszyć wartości współczynników, ale nie eliminuje ich całkowicie.\n",
    "\n",
    "Regresja Lasso dodaje termin kary na podstawie wartości bezwzględnych współczynników, co może skutkować wartościami zerowymi dla niektórych współczynników, co pozwala na selekcję cech.\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. Zmniejszone nadmierne dopasowanie:\n",
    "1. Uogólnienie poprawy umiejętności\n",
    "1. Wybór najważniejszych predyktorów\n",
    "\n",
    "Wady \n",
    "\n",
    "1. Utrata możliwości interpretacji.\n",
    "1. Zależność dostrajania hiperparametrów\n",
    "1. Utrata precyzji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18daabb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "087587c8",
   "metadata": {},
   "source": [
    "5. Regresja nieliniowa: modele modelujące nieliniową zależność między cechami wejściowymi a zmienną wyjściową przy użyciu nieliniowych funkcji lub algorytmów, takich jak neuron\n",
    "\n",
    "Wzór na regresję nieliniową może zależeć od konkretnego wybranego założenia modelu nieliniowego. Jednak ogólnie wzór na regresję nieliniową jest następujący:\n",
    "\n",
    "y = f(θ₁x₁ + θ₂x₂ + ... + θₚxₚ),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "y - zmienna zależna (wartość wyjściowa),\n",
    "\n",
    "x₁, x₂, ..., xₚ są zmiennymi niezależnymi,\n",
    "\n",
    "θ₁, θ₂, ..., θₚ - parametry modelu,\n",
    "\n",
    "f jest funkcją nieliniową (np. wielomianową, wykładniczą, logarytmiczną itp.).\n",
    "\n",
    "W regresji nieliniowej funkcję f można wybrać zgodnie z oczekiwaną zależnością między zmiennymi niezależnymi i zależnymi. Na przykład, jeśli przyjmuje się zależność wielomianową, funkcja f może być wielomianem stopnia większego niż 1. Jeśli przyjmuje się zależność wykładniczą, funkcja f może być wykładnicza. I tak dalej.\n",
    "\n",
    "Należy zauważyć, że wybór odpowiedniej funkcji nieliniowej f wymaga wstępnej analizy danych i zrozumienia natury zależności między zmiennymi. Wyznaczenie odpowiedniej funkcji nieliniowej można przeprowadzić empirycznie, na podstawie eksperymentów lub z wykorzystaniem metod statystycznych i uczenia maszynowego.\n",
    "\n",
    "Estymacja parametrów modelu (θ) w regresji nieliniowej może odbywać się metodą najmniejszych kwadratów, metodą największej wiarygodności lub innymi metodami optymalizacyjnymi, w zależności od wybranego modelu i preferencji badacza.\n",
    "\n",
    "Zalety \n",
    "\n",
    "1. Elastyczność modelu: Regresja nieregularna umożliwia modelowanie złożonych nieliniowych relacji między predyktorami a zmienną zależną.\n",
    "\n",
    "2. Lepsze dopasowanie danych: uwzględnienie funkcji i przekształceń nieliniowych umożliwia dokładniejsze dopasowanie modelu do danych, zwłaszcza w przypadkach, gdy związek między zmiennymi jest nieliniowy.\n",
    "\n",
    "3. Możliwość modelowania złożonych interakcji: Regresja nieregularna może uchwycić złożone interakcje między predyktorami, co pozwala na dodatkową złożoność danych i poprawia moc predykcyjną modelu.\n",
    "\n",
    "Wady \n",
    "\n",
    "1. Trudność w interpretacji: Włączenie nieliniowych cech i przekształceń komplikuje interpretację modelu. Współczynniki nie mają już bezpośredniego znaczenia, jak w regresji liniowej, a ich interpretacja może być trudna.\n",
    "\n",
    "2. Nadmierne dopasowanie: w przypadku zbyt dużej elastyczności modelu lub niewystarczających danych nieregularna regresja może cierpieć z powodu nadmiernego dopasowania.\n",
    "\n",
    "3. Złożoność obliczeniowa: uwzględnienie funkcji nieliniowych może zwiększyć złożoność obliczeniową uczenia modelu. Szczególnie w przypadku korzystania ze złożonych funkcji nieliniowych lub dużej liczby predyktorów uczenie może wymagać dużej mocy obliczeniowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7402b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d674a4",
   "metadata": {},
   "source": [
    "Metoda najmniejszych kwadratów (OLS) to metoda statystyczna, która służy do oszacowania optymalnych wartości współczynników (θ) w regresji liniowej. Celem OLS jest zminimalizowanie sumy kwadratów odchyleń (błędów) między wartościami rzeczywistymi a wartościami przewidywanymi modelu.\n",
    "\n",
    "Chcemy znaleźć optymalne wartości współczynników θ, aby zminimalizować sumę błędów kwadratowych (SSE), która wyraża się następująco:\n",
    "\n",
    "$ SSE = Σ(yi - h(xi))^2 $\n",
    "\n",
    "gdzie h(xi) jest przewidywaną wartością zależną od cech wejściowych xi i wag θ:\n",
    "\n",
    "Gdzie $ h(xi) = θ0 + θ1x1 + θ2x2 + ... + θn*xn $ model regresji liniowej, który faktycznie przewiduje xi h(xi) to y_pred\n",
    "\n",
    "LSM pozwala uzyskać optymalne wartości współczynników θ, które zapewniają najlepsze dopasowanie modelu do danych zbioru uczącego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130e14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992f3fd0",
   "metadata": {},
   "source": [
    "Gradient Descent to algorytm optymalizacji używany do znalezienia minimum (lub maksimum) funkcji. Jest szeroko stosowany w uczeniu maszynowym do uczenia modeli i dostrajania ich parametrów.\n",
    "\n",
    "Ideą metody zejście gradientu jest sekwencyjne aktualizowanie wartości parametrów modelu w kierunku przeciwnym do gradientu funkcji strat. \n",
    "\n",
    "Gradient funkcji wskazuje kierunek najostrzejszego wzrostu funkcji, a gradient przeciwny wskazuje kierunek najszybszego spadku funkcji. Dlatego poruszanie się w kierunku przeciwnym do gradientu pozwala zbliżyć się do minimum funkcji.\n",
    "\n",
    "W kontekście regresji liniowej zejście gradientu może być wykorzystane do dostrojenia optymalnych wartości parametrów (θ) modelu poprzez minimalizację sumy błędów kwadratowych (SSE) między wartościami rzeczywistymi i przewidywanymi. Przy każdej iteracji zejście gradientu wartości parametrów są aktualizowane zgodnie z gradientem funkcji strat i zadaną szybkością uczenia.\n",
    "\n",
    "Podstawowe etapy metody zejścia gradientowego to:\n",
    "1. Inicjalizacja wartości początkowych parametrów modelu.\n",
    "2. Obliczenie przewidywanych wartości modelu na bieżących parametrach.\n",
    "3. Obliczanie błędów między wartościami przewidywanymi a rzeczywistymi.\n",
    "4. Obliczanie gradientów funkcji strat dla każdego parametru.\n",
    "5. Zaktualizuj wartości parametrów w kierunku przeciwnym do gradientu, uwzględniając tempo uczenia się.\n",
    "6. Powtarzaj kroki 20-50 aż do osiągnięcia warunku zatrzymania (np. osiągnięcia określonej liczby iteracji lub wystarczająco małej zmiany funkcji straty).\n",
    "\n",
    "formuła gradientu zejście:\n",
    "\n",
    "θₙₑ𝑤 = θₙₑ𝑤 - α * ∇J(θₙₑ𝑤),\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "θₙₑ𝑤 - aktualna wartość parametru,\n",
    "α - szybkość uczenia się,\n",
    "∇J(θₙₑ𝑤) jest gradientem funkcji straty J względem parametru θₙₑ𝑤.\n",
    "\n",
    "Gradient (∇J(θₙₑ𝑤)) jest wektorem zawierającym pochodne cząstkowe funkcji straty względem każdego parametru modelu. Każda składowa gradientu wskazuje kierunek najszybszego wzrostu funkcji straty w punkcie θₙₑ𝑤.\n",
    "\n",
    "Proces zejście gradientu polega na iteracyjnym aktualizowaniu parametrów modelu przy użyciu wzoru na zejście gradientu. Na każdym kroku obliczamy gradient funkcji straty w stosunku do bieżących wartości parametrów, następnie mnożymy go przez szybkość uczenia i odejmujemy wynik od bieżących wartości parametrów. Idziemy więc w kierunku najszybszego spadku funkcji straty.\n",
    "\n",
    "Wybór odpowiedniego współczynnika uczenia się (α) jest ważnym aspektem zejście gradientu. Jeśli α jest zbyt małe, konwergencja może być powolna. Jeśli α jest zbyt duże, zejście może się nie zbiegać i przekroczyć minimum funkcji straty. Wybór optymalnej wartości współczynnika uczenia jest problemem optymalizacyjnym.\n",
    "\n",
    "Metoda zejście gradientu może mieć różne odmiany, takie jak zejście gradientu stochastycznego i zejście gradientu mini-batch, które wykorzystują losowe podzbiory danych do aktualizacji parametrów w każdej iteracji. Te odmiany poprawiają szybkość konwergencji i wydajność metody na dużych ilościach danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4e202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47f92881",
   "metadata": {},
   "source": [
    "Intercept (punkt przecięcia) w kontekście regresji liniowej jest wolnym elementem modelu, reprezentującym wartość zmiennej zależnej (wartości wyjściowej) przy zerowych wartościach wszystkich cech wejściowych.\n",
    "\n",
    "W regresji liniowej model jest reprezentowany przez równanie:\n",
    "\n",
    "$ y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ $\n",
    "\n",
    "gdzie y - zmienna zależna, θ₀ - wyraz wolny, θ₁, θ₂, ..., θₚ - współczynniki regresji, x₁, x₂, ..., xₚ - cechy wejściowe.\n",
    "\n",
    "Punkt przecięcia (punkt przecięcia) to punkt przecięcia linii regresji z osią y przy zerowych wartościach wszystkich cech wejściowych. Odzwierciedla wartość bazową zmiennej zależnej, gdy wszystkie cechy wejściowe są zerowe.\n",
    "\n",
    "Punkt przecięcia może być dodatni lub ujemny i reprezentuje przesunięcie linii regresji w górę lub w dół osi y.\n",
    "\n",
    "Punkt wolny odgrywa ważną rolę w modelu regresji liniowej, ponieważ pozwala uwzględnić wartość bazową zmiennej zależnej, która nie zależy od cech wejściowych. Włączenie punktu przecięcia umożliwia modelowanie linii regresji, która niekoniecznie przechodzi przez początek układu współrzędnych.\n",
    "\n",
    "Punkt przecięcia (θ₀) jest jednym z parametrów estymowanych w procesie uczenia regresji liniowej i wpływa na kształt i położenie linii regresji w przestrzeni cech."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
